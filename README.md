# Project- Hadoop + PySpark(Big Data)
  
Setul de date utilizat este Walmart Recruiting - Prognoza vânzărilor în magazin  
Setul de date curatat poate fi descarcat accesand linkul:  
   https://drive.google.com/file/d/1CYkbzqXwAu83SSg4GurgQKAunIC8A09J/view?usp=sharing   


# Cerințele proiectui Hadoop + Spark

Cerințele sunt:  
1.	Minim cate un pic din fiecare (nota 5+):  
•	Sa aveți un cluster de minim 2 noduri (plus master) funcțional pe RaaS-IS (conform cu tutorialul prezentat la curs si pe Drive)  
•	Sa încărcați datele folosite in HDFS (hdfs dfs -put … | mkdir | etc.)  
•	Sa porniti masterul de Spark + workers (arm-env.sh + master:8080 in browser si celelalte servicii)  
•	Sa folosiți Jupyter Lab pentru notebooks  
•	Sa executați orice metoda specifica pentru Exploratory Data Analysis (min, max, sum, stdev, etc) + grafice (in orice librarie python!)  
•	Sa executați minim 3-5 interogări pe tabelele încărcate in HDFS/Spark  
•	Înțelege in mare ecosistemul Spark + HDFS  

2.	Punctez in plus daca:  
•	Exploratory Data Analysis este mai complex, grafice mai complexe  
•	Folosiți intre 3-5+ tabele încărcate din HDFS  
•	A generat intre 5-10 interogări (complexitate mai mare)  
•	Salveaza rezultatele interogarilor in HDFS (folder nou/fisiere)  
•	Folosiți si alte formate de date in HDFS precum Parquet, Avro, ORC – se poate converti chiar din Spark (google it!)  


