# Project- Hadoop + PySpark(Big Data)
  
The dataset used is Walmart Recruiting - Store Sales Forecast  
The cleaned dataset can be downloaded by accessing the link:
   https://drive.google.com/file/d/1CYkbzqXwAu83SSg4GurgQKAunIC8A09J/view?usp=sharing   


# Requirements for the Hadoop + Spark project  

The requirements are:  
1.	At least a bit of everything (grade 5+):  
•	Have a functional cluster with at least 2 nodes (plus master) on RaaS-IS (as per the tutorial presented in class and on Drive)  
•	Upload the data used into HDFS (hdfs dfs -put … | mkdir | etc.)  
•	Start the Spark master + workers (arm-env.sh + master:8080 in browser and other services)  
•	Use Jupyter Lab for notebooks  
•	Execute any specific method for Exploratory Data Analysis (min, max, sum, stdev, etc.) + charts (in any Python library!)  
•	Execute at least 3–5 queries on the tables loaded into HDFS/Spark  
•	Have a basic understanding of the Spark + HDFS ecosystem  

2. Extra points if:  
•	You perform a more complex Exploratory Data Analysis, with more advanced charts  
•	You use between 3–5+ tables loaded from HDFS  
•	You generate between 5–10 queries (with higher complexity)  
•	You save the query results in HDFS (new folder/files)  
•	You use other data formats in HDFS, such as Parquet, Avro, ORC – can be converted directly from Spark (Google it!)  





